<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Eigengoogle: How the Google PageRank Algorithm Works</title>
  
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-89673307-1', 'auto');
ga('send', 'pageview');
</script>


  <script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  <script>
    MathJax.Hub.Config({
      preRemoveClass: "mj",
      tex2jax: {
        inlineMath: [['$','$']],
        processEscapes: true
      },
      CommonHTML: { linebreaks: { automatic: true } },
      "HTML-CSS": { linebreaks: { automatic: true } },
             SVG: { linebreaks: { automatic: true } }
    });
  </script>


	
	
	<link rel="stylesheet" href="/css/bootstrap-grid.css">
	<link rel="stylesheet" href="/css/style.css">
	
	<meta name="generator" content="Hugo 0.40.1" />
</head>
<body>
  <div class="container">
    <header>
      <h1 class="main-title"><a href="/">Noam&#39;s Website About Software</a></h1>
    </header>

  <div>
    
      <small>
      <strong>Back to wiki:</strong>
      
        
        <a href="/wikis/datascience">datascience</a>
      
        ,
        
        <a href="/wikis/math">math</a>
      
      </small>
    
  </div>

  
    <main>
  
		<article>
			<h1 class="article-title">Eigengoogle: How the Google PageRank Algorithm Works</h1>
      <div class="article-date">
        Published: <time>2014-01-27</time>
      </div>
			<div>
				

<p>While we&rsquo;re on the subject of sorting things online, we might as well talk
about Google: the 93-billion dollar company whose main export is taking all the
things ever and putting them in the right order. If there&rsquo;s one thing Google
knows best, it&rsquo;s sorting stuff.</p>

<p>I was curious how it all works, and it turned out really interesting, plus I
got to learn a bit about Markov chains. It all starts with an algorithm called
PageRank<sup>1</sup>.
<a href="http://en.wikipedia.org/wiki/PageRank">Accodring to Wikipedia</a>:</p>

<blockquote>
<p>Pagerank uses a model of a random surfer who gets bored after several clicks
and switches to a random page. It can be understood as a Markov chain in
which the states are pages, and the transitions are the links between pages.
When calculating PageRank, pages with no outbound links are assumed to link
out to all other pages in the collection (the random surfer chooses another
page at random).</p>

<p>The PageRank values are the entries of the dominant eigenvector of the
modified adjacency matrix.</p>
</blockquote>

<p><img src="/img/eigenvectors.png" alt="" /></p>

<p>In this post I&rsquo;ll try to break that down and provide some of the background
necessary to understand Google PageRank. My main reference for making sense
of it all was the book
<em>Probability, Markov Chains, Queues, And Simulation</em> by William J. Stweart, my
linear algebra textbook, and Google.</p>

<h2 id="graphs-as-matrices">Graphs as Matrices</h2>

<p>A graph is a collection of nodes joined by edges. If the edges are arrows that
flow in one direction, we call that a <strong>directed graph</strong>. A graph whose
edges have each been assigned a &ldquo;weight&rdquo; (usually some real number) is a
<strong>weighted graph</strong>.</p>

<p><img src="/img/weighted_graph01.png" alt="" /></p>

<p>A graph of <code>n</code> nodes can be represented in the form of an <code>n x n</code> <strong>adjacency
matrix</strong>, $M = [m_{ij}]$% such that $m_{ij}$ is equal to the weight of the edge
going from node $j$ to node $i$:</p>

<pre><code>[0, 1, 0, 0]
[1, 0, 2, 0]
[2, 1, 0, 1]
[0, 0, 4, 0]
</code></pre>

<h1 id="stochastic-matrices">Stochastic Matrices</h1>

<p>The term &ldquo;stochastic&rdquo; is used to describe systems whose state can only be
described in probabilistic terms (i.e: the likelihood of some event happening
at any given time).</p>

<blockquote>
<p><strong>Scenario</strong>:</p>

<p>Consider two competing websites. Every month, the first website loses 30% of
its audience to the second website, while the second website loses 60% of its
audience to the first.</p>

<p>If the two websites start out with 50% of the global audience each, how many
users will each website have after a month? After a year?</p>
</blockquote>

<p>This scenario can be represented as the following system:</p>

<pre><code>P = [0.7, 0.6],   x_0 = [0.5, 0.5]
    [0.3, 0.4]
</code></pre>

<p><img src="/img/competing_stores_graph.png" alt="" /></p>

<p>This is a <strong>Markov chain</strong> with <strong>transition matrix</strong> $P$ and a <strong>state vector</strong>
$\mathbf{ x^{(0)} }$.</p>

<p>The transition matrix is called a <strong>stochastic matrix</strong>; it represents the
likelihood that some individual in a system will transition from one state to
another. The columns on a stochastic matrix are always non-negative numbers
that add up to 1 (i.e: the probability of <strong>at least one</strong> of the events
occurring is always 1 &ndash; the likelihood of a user either staying on the same
website, or leaving, is always 100%. He must choose one of the two).</p>

<p>The state after the first month is</p>

<p>$$
  \begin{gather}
    \mathbf{x^{ (1) }} = P \mathbf{ x^{ (0) } } \cr
      = [(0.7 + 0.6)\times0.5, (0.3 + 0.4)\times0.5] \cr
      = [0.65, 0.35] \cr
  \end{gather}
$$</p>

<p>So, after the first month, the second website will have only 35% of the global audience.</p>

<p>To get the state of the system after two months, we simply apply the transition
matrix again, and so on. That is, the current state of a Markov chain depends
only on its previous state. Thus, the state vector at month $k$ can be
defined recursively:</p>

<p>$$ \mathbf{ x^{(k)} } = P\mathbf{ x^{ (k - 1) } } $$</p>

<p>From which, through substitution, we can derive the following equation:</p>

<p>$$\mathbf{ x^{(k)} } = P^k \mathbf{ x^{(0)} }$$</p>

<p>Using this information, we can figure out the state of the system after a year,
and then again after two years (using the (Sage)[<a href="http://www.sagemath.org/">http://www.sagemath.org/</a>], a
mathematical library for python):</p>

<pre><code>P = Matrix([[0.70, 0.60], [0.30, 0.40]])
x = vector([0.5,0.5])

P^12 * x
# -&gt; (0.666666666666500, 0.333333333333500)

P^24 * x
# -&gt; (0.666666666666666, 0.333333333333333)
</code></pre>

<p>So it seems like the state vector is &ldquo;settling&rdquo; around those values. It would
appear that, as $n \to \infty$, $P^n\mathbf{x^{(0)}}$ is converging to some
$\mathbf{x}$ such that $P\mathbf{x} = \mathbf{x}$.
As we&rsquo;ll see below, this is indeed the case.</p>

<p>We&rsquo;ll call this $\mathbf{x} $ the <strong>steady state vector</strong>.</p>

<h2 id="eigenvectors">Eigenvectors!</h2>

<p>Recall from linear algebra that an eigenvector of a matrix $A$ is a vector
$\mathbf{x}$ such that:</p>

<p>$$A\mathbf{x} = \lambda \mathbf{x}$$</p>

<p>for some scalar $\lambda$ (the <strong>eigenvalue</strong>). A <strong>leading eigenvalue</strong> is an
eigenvalue $\lambda_{1}$ such that its absolute value is greater than any
other eigenvalue for the given matrix.</p>

<p>One method of finding the leading eigenvector of a matrix is through a
<a href="http://en.wikipedia.org/wiki/Power_iteration">power iteration</a> sequence, defined
recursively like so:</p>

<p>$$
  \mathbf{ x_k }
    = \cfrac{A\mathbf{x_{k-1}}}
            {| A\mathbf{x_{ k-1 }} |}
$$</p>

<p>Again, by noting that we can substitute
$A\mathbf{x_{k-1}} = A(A\mathbf{x_{k-2}}) = A^2\mathbf{x_{k-2}}$,
and so on, it follows that:</p>

<p>$$
  \mathbf{x_k}
    = \cfrac{A^k \mathbf{x_0}}
            {| A^k \mathbf{x_0} |}
$$</p>

<p>This sequence converges to the leading eigenvector of $A$.</p>

<p>Thus we see that the steady state vector is just an eigenvector with the special
case $\lambda = 1$.</p>

<h2 id="stochastic-matrices-that-don-t-play-nice">Stochastic Matrices that Don&rsquo;t Play Nice</h2>

<p>Before we can finally get to Google PageRank, we need to make a few more
observations.</p>

<p>First, it should be noted that power iteration has its limitations: not all
stochastic matrices converge. Take as an example:</p>

<pre><code>P = Matrix([ [0, 1, 0], [1, 0, 0], [0, 0, 1]])
x = vector([0.2, 0.3, 0.5])

P * x
# -&gt; (0.3, 0.2, 0.5)

P^2 * x
# -&gt; (0.2, 0.3, 0.5)

P^3 * x
# -&gt; (0.3, 0.2, 0.5)
</code></pre>

<p>The state vectors of this matrix will oscillate in such a way forever. This
matrix can be thought of as the transformation matrix for reflection about a
line in the x,y axis&hellip; this system will never converge (indeed, it has no
leading eigenvalue: $|\lambda_1| = |\lambda_2| = |\lambda_3| = 1$).</p>

<p>Another way of looking at $P$ is by drawing its graph:</p>

<p><img src="/img/oscillating_chain.png" alt="" /></p>

<p>Using our example of competing websites, this matrix describes a system such
that, every month, <em>all</em> of the first website&rsquo;s users leave and join the
seconds website, only to abandon the second website again a month later and
return to the first, and so on, forever.</p>

<p>It would be absurd to hope for this system to converge to a steady state.</p>

<p>States 1 and 2 are examples of <strong>recurrent states</strong>. These are states that,
once reached, there is a probability of 1 (absolute certainty) that the Markov
chain will return to them infinitely many times.</p>

<p>A <strong>transient state</strong> is such that the probability is $&gt; 0$ that they will
never be reached again. (If the probability <em>is</em> 0, we call such a state
<strong>ephemeral</strong> &ndash; in terms of Google PageRank, this would be a page that no
other page links to):</p>

<p><img src="/img/diffrent_states.png" alt="" /></p>

<p>There are two conditions a transition matrix must meet if we want to ensure
that it converges to a steady state:</p>

<p>It must be <strong>irreducible</strong>: an irreducible transition matrix is a matrix
whose graph has no closed subsets. (A closed subset is such that no state
within it can reach a state outside of it. 1, 2 and 3 above are closed from 4
and 5.)</p>

<p>It must be <strong>primitive</strong>: A primitive matrix $P$ is such that, for some
positive integer $n$, $P^n$ is such that $p_{ij} &gt; 0$ for all
$p_{ij} \in P$ (that is: all of its entries are positive numbers).</p>

<blockquote>
<p>More generally, it must be <strong>positive recurrent</strong> and <strong>aperiodic</strong>.</p>

<p>Positive recurrence means that it takes, on average, a finite number of steps
to return to any given state. Periodicity means the number of steps it takes
to return to a particular state is always divisible by some natural number
$n$ (its period).</p>

<p>Since we&rsquo;re dealing with finite Markov chains, irreducibility implies
positive recurrence, and primitiveness ensures aperiodicity.</p>
</blockquote>

<p><img src="/img/periodic.png" alt="" /></p>

<h2 id="google-pagerank">Google PageRank</h2>

<p>We are now finally ready to understand how the PageRank algorithm works. Recall
from Wikipedia:</p>

<blockquote>
<p>The formula uses a model of a random surfer who gets bored after several
clicks and switches to a random page. The PageRank value of a page reflects
the chance that the random surfer will land on that page by clicking on a
link. It can be understood as a Markov chain in which the states are pages,
and the transitions, which are all equally probable, are the links between
pages.</p>
</blockquote>

<p>So, for example, if we wanted to represent our graph above, we would start with
the following adjacency matrix:</p>

<pre><code>[0, 0, 0.5, 0,   0],
[0, 0, 0.5, 0.5, 0],
[1, 1, 0,   0,   0],
[0, 0, 0,   0,   0],
[0, 0, 0,   0.5, 0]
</code></pre>

<p>For the algorithm to work, we must transform this original matrix in such a way
that we end up with an irreducible, primitive matrix. First,</p>

<blockquote>
<p>If a page has no links to other pages, it becomes a sink and therefore
terminates the random surfing process. If the random surfer arrives at a sink
page, it picks another URL at random and continues surfing again.</p>

<p>When calculating PageRank, pages with no outbound links are assumed to link out
to all other pages in the collection.</p>
</blockquote>

<pre><code>    [0, 0, 0.5, 0,   0.2],
    [0, 0, 0.5, 0.5, 0.2],
S = [1, 1, 0,   0,   0.2],
    [0, 0, 0,   0,   0.2],
    [0, 0, 0,   0.5, 0.2]
</code></pre>

<p>We are now ready to produce $G$, the Google Matrix, which is both irreducible
and primitive. Its steady state vector gives us the final PageRank score for
each page.</p>

<h2 id="the-google-matrix">The Google Matrix</h2>

<p>The [Google Matric](<a href="http://en.wikipedia.org/wiki/Google_matrix]">http://en.wikipedia.org/wiki/Google_matrix]</a> for an $n
\times n$ matrix $S$ is derived from the equation</p>

<p>$$ G = \alpha S + (1 - \alpha) \frac{1}{n} E$$</p>

<p>Where $E = \mathbf{ e }\mathbf{ e }^T$ is an $n \times n$
matrix whose entries are all 1, and
$0 \le \alpha \le 1$ is referred to as the <strong>damping factor</strong>.</p>

<p>If $\alpha = 1$, then $G = S$. Meanwhile, if $\alpha = 0$ all of the entries in
$G$ are the same (hence, the original structure of the network is &ldquo;dampened&rdquo; by
$\alpha$, until we lose it altogether).</p>

<p>So the matrix $(1 - \alpha) \frac{1}{n} E$ is a matrix that represents a &ldquo;flat&rdquo;
network in which all pages link to all pages, and the user is equally likely to
click any given link (with likelihood $\frac{1-\alpha}{n}$), while $S$ is
dampened by a factor of $\alpha$.</p>

<blockquote>
<p>Google uses a damping factor of 0.85. For more on this, I found <a href="/eigenpaper">this
paper</a>.</p>

<p><strong>tl;dr:</strong> the second eigenvalue of a Google matrix is $|\lambda_2| = \alpha
\le |\lambda_1| = 1 $, and the rate of convergence of the power iteration is
given by $\frac{|\lambda_2|}{|\lambda_1|} = \alpha$. So higher values of
$\alpha$ imply better accuracy but worse performance.</p>
</blockquote>

<p>With some moving stuff around, we can see that</p>

<p>$$
  \left(\alpha s_{ 1j } + \frac{1-\alpha}{ n }\right) + \left(\alpha s_{ 2j }
  + \frac{1-\alpha}{ n }\right) + &hellip; + \left(\alpha s_{ nj } + \frac{1-\alpha}{ n }\right) = 1
$$</p>

<p>For all $j$ up to $n$, which means that $G$ is indeed stochastic, irreducible,
and primitive. Cool.</p>

<p>In conclusion,</p>

<p><img src="/img/eigensnotsicles.png" alt="" /></p>

<hr />

<p><small>
  1. Actually, it all started with the <a href="http://en.wikipedia.org/wiki/HITS_algorithm">HITS algorithm</a>, which PageRank is based off of. More details <a href="http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture4/lecture4.html">here</a>.
</small></p>

			</div>
		</article>

    <hr>

    <div>
      
        <strong>Tags:</strong>
        
          
          <a href="/tags/featured">featured</a>
        
          ,
          
          <a href="/tags/linearalgebra">linearalgebra</a>
        
          ,
          
          <a href="/tags/markovchains">markovchains</a>
        
          ,
          
          <a href="/tags/probability">probability</a>
        
      
    </div>

    <div>
      
    </div>

    <hr>

    <div>
      
    </div>
	</main>

    <footer>
      <p>&copy; 2018 <a href="http://noamswebsite.com/">Noam
      Gagliardi-Rabinovich</a></p>

      <p>
        <small>
          Website is built with Hugo static site generator. Full source
          available <a
          href="here](https://github.com/noam87/noamswebsite">here</a> and
          generated HTML available
          <a href="https://github.com/noam87/noam87.github.io">here</a>.
        </small>
      </p>
    </footer>
  </div>
</body>
</html>

